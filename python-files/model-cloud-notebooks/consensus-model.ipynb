{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d18236-02cd-407c-83ac-4e5893d99a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 21:08:16.996456: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, f1_score, accuracy_score, roc_auc_score, roc_curve, auc, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317a5627-7016-43f9-9204-dadd63950df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to import models\n",
    "def get_exported_models(import_dir):\n",
    "    \"\"\"\n",
    "     This function takes the path to\n",
    "     the import directory\n",
    "     for each lagged model\n",
    "    \"\"\"\n",
    "    # initialize empty list to return\n",
    "    models = []\n",
    "    # loop over all the files in the import directory\n",
    "    # we can use the listdir() method on the os class\n",
    "    # this lists all the files and sub-directories\n",
    "    for file in os.listdir(import_dir):\n",
    "        # make sure the file name ends with .pkl\n",
    "        if file.endswith('.pkl'):\n",
    "            # get the file name\n",
    "            # this returns tuple of file name and extension\n",
    "            # so we pick [0] to get the file name\n",
    "            file_name = os.path.splitext(file)[0]\n",
    "            # read the file and append\n",
    "            # get full file path\n",
    "            # we need to do this to read from the pickle file\n",
    "            filepath = os.path.join(import_dir, file)\n",
    "            # call pickle to extract file\n",
    "            # we now use rb to read binary since we exported as bin\n",
    "            with open(filepath, \"rb\") as f:\n",
    "                # load the model\n",
    "                model = pickle.load(f)\n",
    "            # append to list\n",
    "            models.append((file_name, model))\n",
    "    # return the list\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c624ec9-8376-44aa-a238-041e925fd986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 21:13:56.431180: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open ram://9139cb17442b43a581bcb423697f09ad: INVALID_ARGUMENT: ram://9139cb17442b43a581bcb423697f09ad is a directory.\n",
      "2024-04-25 21:13:56.578724: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open ram://ac9e780d0a344311bedd0dba484dca4b: INVALID_ARGUMENT: ram://ac9e780d0a344311bedd0dba484dca4b is a directory.\n",
      "2024-04-25 21:13:56.730449: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open ram://9e8629867d614772a986fb81d03195d8: INVALID_ARGUMENT: ram://9e8629867d614772a986fb81d03195d8 is a directory.\n",
      "2024-04-25 21:13:56.874062: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open ram://5e41f4bc0e754319990a7765662dd281: INVALID_ARGUMENT: ram://5e41f4bc0e754319990a7765662dd281 is a directory.\n",
      "2024-04-25 21:13:57.019434: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open ram://fd1253e7f0b44821b97f855c3c84de18: INVALID_ARGUMENT: ram://fd1253e7f0b44821b97f855c3c84de18 is a directory.\n"
     ]
    }
   ],
   "source": [
    "# import all models\n",
    "#-----------------#\n",
    "\n",
    "# import logit models\n",
    "logit_import_dir = \"[ENTER PATH FOR LOGIT MODELS]\"\n",
    "# run and extract\n",
    "logit_models = get_exported_models(logit_import_dir)\n",
    "\n",
    "# import random forest models\n",
    "random_forest_import_dir = \"[ENTER PATH FOR RANDOM FOREST MODELS]\"\n",
    "# run and extract\n",
    "random_forest_models = get_exported_models(random_forest_import_dir)\n",
    "\n",
    "# import the nn models\n",
    "nn_import_dir = \"[ENTER PATH FOR NEURAL NETWORK MODELS]\"\n",
    "# run and extract\n",
    "nn_models = get_exported_models(nn_import_dir)\n",
    "\n",
    "# import the svc models\n",
    "svc_import_dir = \"[ENTER PATH FOR SVC MODELS]\"\n",
    "# run and extract\n",
    "svc_models = get_exported_models(svc_import_dir)\n",
    "\n",
    "# import gradient boosting models\n",
    "gradient_boosting_models = \"[ENTER PATH FOR XGBOOST MODELS]\"\n",
    "# run and extract\n",
    "gradient_boosting_models = get_exported_models(gradient_boosting_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d053203b-7f72-4899-a23e-52807dd80071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and load data\n",
    "#--------------------#\n",
    "\n",
    "# import raw data for testing\n",
    "data_raw = pd.read_excel('[ENTER PATH FOR THE DATA]', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d58dc11e-dab2-46e3-9cba-de5b0e9a6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up function to loop over and test different\n",
    "# thresholds then extract the best one\n",
    "def find_best_threshold(y_predicted_raw, y_true):\n",
    "    # create range from 0-1\n",
    "    # in increments of 0.0001\n",
    "    threshold_intervals = np.arange(0, 1.0001, 0.0001)\n",
    "    # now loop over each interval and run the model\n",
    "    # to see which one performs best\n",
    "    # create list to store the results\n",
    "    results = []\n",
    "    for threshold in threshold_intervals:\n",
    "        # compare at each threshold\n",
    "        y_pred_at_threshold = (y_predicted_raw > threshold).astype(int)\n",
    "        # calculate the f1 score for the predicted\n",
    "        threshold_f1 = f1_score(y_true, y_pred_at_threshold)\n",
    "        # create tuple of each threshold and f1\n",
    "        results.append((threshold_f1, y_pred_at_threshold, threshold))\n",
    "    # find the threshold with the max f1\n",
    "    max_threshold_tuple = max(results, key=lambda x: x[0])\n",
    "    # now extract the threshold from tuple\n",
    "    best_threshold = max_threshold_tuple[2]\n",
    "    y_pred = max_threshold_tuple[1]\n",
    "    best_f1 = max_threshold_tuple[0]\n",
    "    # return the result\n",
    "    return [best_threshold, y_pred, best_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "538d72f8-4f56-43dd-8096-fd9e2529e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to store the exported figures\n",
    "# roc_curve plots\n",
    "roc_fig_dir_path = '/Users/jean-philippbruehwiler/Desktop/master-thesis-code/model-plots/cm-plots/cm-roc-plots/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09a5126e-fad5-4e05-b59c-75210dfc1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to create roc_curve\n",
    "def roc_curve_plot(y_true, y_pred_prob, lag, fig_dir_path, model_name):\n",
    "    # get the fpr, trp, and thresholds\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n",
    "    # call method on fpr and tpr\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # create plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # we insert the area under curve up to 2 decimal places\n",
    "    # we can insert using the modulo operator - %\n",
    "    plt.plot(fpr, tpr, color='lightblue', linewidth=2, label='ROC Curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='darkred', linewidth=2, linestyle=':')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f\"Logit ROC Curve for {lag} Month Lag\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    file_name = f\"roc_curve_{model_name}_lag_{lag}.png\"\n",
    "    save_path = os.path.join(fig_dir_path, file_name)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48cc9f40-3c8d-40cd-93a0-2918b2d73543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to flatten list of nested lists\n",
    "# the predicted values are returned like this:\n",
    "# i.e. [[0],[1],[0],[1]]\n",
    "# we need to convert it to this:\n",
    "# i.e. [0,1,0,1]\n",
    "def flatten_list(matrix_list):\n",
    "    flat_list = []\n",
    "    for row in matrix_list:\n",
    "        flat_list.append(row[0])\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7876ab34-fd93-464f-a51f-f0d2678a35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up helper function to extract the correct model\n",
    "# based on the lag, this takes the list of a certain\n",
    "# model and returns the model\n",
    "def extract_correct_lag_model(models, lag):\n",
    "    \"\"\"This function take models list\n",
    "       and the lag to match\"\"\"\n",
    "    # create regex pattern based on lag\n",
    "    lag_match_pattern = rf\"{lag}-month-lag\"\n",
    "    # loop over models\n",
    "    for model in models:\n",
    "        # use .search() method to check the entire string\n",
    "        # check the first part of tuple for name\n",
    "        # then return second part with the model\n",
    "        # tuple: (file name of model, model.pkl file)\n",
    "        if re.search(lag_match_pattern, model[0]):\n",
    "            return model[1]\n",
    "    # if no model is found return none\n",
    "    # this should never be triggered but good\n",
    "    # to include just in case\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2b9ee36-6e92-4e1b-b0c0-69c9dff38f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find average for row\n",
    "def avg_pred_prob(row):\n",
    "    return row.sum() / len(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f80416a4-a166-482c-a474-4f54521be487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up lags for looping\n",
    "lags = [3, 6, 9, 12, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "60c7206a-3bae-40de-b506-4c517dd51c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the models are already trained so we just need to test the models\n",
    "# create function to run\n",
    "def run_consensus_model(data, lag, test_size):\n",
    "    \"\"\"This model accepts testing data and a lag\n",
    "       it then uses a simple voting concensus to determine\n",
    "       class, i.e. if a majority of models say 1,\n",
    "       then the model returns 1\"\"\"\n",
    "\n",
    "    # make a copy of the original DataFrame to avoid modifying it\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # modify dataset for lag\n",
    "    # we want to set the recession indicator back by the lag so that t0 is aligned with t+lag\n",
    "    data_copy[f\"nber_recession_{lag}_month_lag\"] = data_copy['nber_recession'].shift(-lag)\n",
    "\n",
    "    # drop the original recession column and na values\n",
    "    data_copy = data_copy.drop(columns=['nber_recession'])\n",
    "    data_copy = data_copy.dropna()\n",
    "\n",
    "    # set up training and testing data\n",
    "    X = data_copy.drop(columns=[f\"nber_recession_{lag}_month_lag\"])\n",
    "    y = data_copy[f\"nber_recession_{lag}_month_lag\"]\n",
    "\n",
    "    # set up training and testing data\n",
    "    # we don't need the trainin data in this case, only the testing data\n",
    "    _, X_test, _, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # extract the relevant model for the given lag\n",
    "    random_forest = extract_correct_lag_model(random_forest_models, lag)\n",
    "    svc = extract_correct_lag_model(svc_models, lag)\n",
    "    gradient_boosting = extract_correct_lag_model(gradient_boosting_models, lag)\n",
    "    neural_network = extract_correct_lag_model(nn_models, lag)\n",
    "    logit_model = extract_correct_lag_model(logit_models, lag)\n",
    "\n",
    "    # make the calculations for each model\n",
    "    rand_forest_pred = random_forest.predict_proba(X_test)[:,1]\n",
    "    svc_pred = svc.predict_proba(X_test)[:,1]\n",
    "    gbc_pred = gradient_boosting.predict_proba(X_test)[:,1]\n",
    "    logit_pred = logit_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # neural network needs to handled a bit differently\n",
    "    # we first predict directly, then flatten list\n",
    "    # of lists which it returns\n",
    "    nn_predictions_prob_raw = neural_network.predict(X_test)\n",
    "    # flatten list of lists\n",
    "    nn_predictions_prob = flatten_list(nn_predictions_prob_raw)\n",
    "\n",
    "    # create a dataframe of the consensus predictions\n",
    "    model_probs = pd.DataFrame({\n",
    "    'rand_forest_pred': rand_forest_pred,\n",
    "    'svc_pred': svc_pred,\n",
    "    'gbc_pred': gbc_pred,\n",
    "    'nn_pred': nn_predictions_prob,\n",
    "    'logit_pred': logit_pred\n",
    "    })\n",
    "\n",
    "    # find the average vote for each row\n",
    "    # use apply to get the average value for each row\n",
    "    # we pass in the function then the row is passed as the arg\n",
    "    model_probs['y_pred_avg'] = model_probs.apply(avg_pred_prob, axis=1)\n",
    "\n",
    "    # now we can compare the accuracy of the consensus model\n",
    "    y_pred_raw = model_probs['y_pred_avg'].values\n",
    "\n",
    "    # extract the data for the best threshold\n",
    "    model_results = find_best_threshold(y_pred_raw, y_test)\n",
    "    # extract the relevant results\n",
    "    threshold = model_results[0]\n",
    "    y_pred = model_results[1]\n",
    "    best_f1 = model_results[2]\n",
    "    \n",
    "    # create a confusion matrix to visualize results\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # make the roc_curve plot\n",
    "    # this does not take into account\n",
    "    # the threshold we ind. calculate\n",
    "    roc_curve_plot(y_test, y_pred_raw, lag, roc_fig_dir_path, 'cm')\n",
    "\n",
    "    # create a confusion matrix to visualize results\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # get predicted values and metrics\n",
    "    metrics_obj = {\n",
    "       'accuracy': accuracy_score(y_test, y_pred),\n",
    "       'precision': precision_score(y_test, y_pred),\n",
    "       'recall': recall_score(y_test, y_pred),\n",
    "       'f1': best_f1,\n",
    "       'roc_auc': roc_auc_score(y_test, y_pred_raw),\n",
    "       }\n",
    "\n",
    "    return {'data': data_copy,\n",
    "            'best_threshold': threshold,\n",
    "            'y_true': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_raw': y_pred_raw,\n",
    "            'confusion_matrix': conf_mat,\n",
    "            'model_metrics': metrics_obj}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ec3c4-4ada-4685-815d-8ba7d9b7ca81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the model for each lag\n",
    "consensus_results = [(f\"{lag}_month_lag_results\", run_consensus_model(data_raw, lag, 0.2)) for lag in lags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8136c497-06e7-4413-91e0-c020f99da275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    lag  accuracy  precision    recall        f1   roc_auc  \\\n",
      "0   3_month_lag_results  0.972973   0.923077  0.800000  0.857143  0.987469   \n",
      "1   6_month_lag_results  0.938776   0.692308  0.947368  0.800000  0.961760   \n",
      "2   9_month_lag_results  0.931507   0.733333  0.647059  0.687500  0.945280   \n",
      "3  12_month_lag_results  0.910959   0.666667  0.818182  0.734694  0.929619   \n",
      "4  18_month_lag_results  0.875862   0.607143  0.708333  0.653846  0.915634   \n",
      "\n",
      "            conf_matrix  threshold  \n",
      "0   [[132, 1], [3, 12]]     0.5746  \n",
      "1   [[120, 8], [1, 18]]     0.4878  \n",
      "2   [[125, 4], [6, 11]]     0.6074  \n",
      "3   [[115, 9], [4, 18]]     0.5792  \n",
      "4  [[110, 11], [7, 17]]     0.4922  \n"
     ]
    }
   ],
   "source": [
    "# make a dataframe of all accuracy results\n",
    "headers_metrics = ['lag', 'accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'conf_matrix', 'threshold']\n",
    "# store the results for each iteration\n",
    "iteration_metrics = []\n",
    "# iterate over results\n",
    "for result in consensus_results:\n",
    "    # extract from the tuple\n",
    "    metrics = result[1]['model_metrics']\n",
    "    # extract each value\n",
    "    values = [val for _, val in metrics.items()]\n",
    "    # insert name of lag\n",
    "    values.insert(0, result[0])\n",
    "    # get the confusion matric\n",
    "    conf_matrix = result[1]['confusion_matrix']\n",
    "    # append to values\n",
    "    values.append(conf_matrix)\n",
    "    # get the best threshold\n",
    "    threshold = result[1]['best_threshold']\n",
    "    # append\n",
    "    values.append(threshold)\n",
    "    # append to the list\n",
    "    iteration_metrics.append(values)\n",
    "# convert to a dataframe\n",
    "metric_data = pd.DataFrame(iteration_metrics, columns=headers_metrics)\n",
    "\n",
    "print(metric_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "833cb12a-baf3-4cb5-bd8d-32ac5b8009f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    lag  recession_true  recession_true_pred  recession_false  \\\n",
      "0   3_month_lag_results              15                   13              133   \n",
      "1   6_month_lag_results              19                   25              128   \n",
      "2   9_month_lag_results              17                   15              129   \n",
      "3  12_month_lag_results              22                   27              124   \n",
      "4  18_month_lag_results              24                   27              121   \n",
      "\n",
      "   recession_false_pred  false_pos_rate  false_neg_rate  \n",
      "0                   135        0.007519        0.200000  \n",
      "1                   122        0.054688        0.052632  \n",
      "2                   131        0.031008        0.352941  \n",
      "3                   119        0.072581        0.181818  \n",
      "4                   118        0.082645        0.291667  \n"
     ]
    }
   ],
   "source": [
    "# go through and see if the model is over or underestimating recessions\n",
    "headers_false_true_summary = ['lag', 'recession_true', 'recession_true_pred', 'recession_false', 'recession_false_pred', 'false_pos_rate', 'false_neg_rate']\n",
    "\n",
    "# store iteration calculations\n",
    "iteration_summaries = []\n",
    "\n",
    "# loop over data\n",
    "for result in consensus_results:\n",
    "    # extract the relevant data\n",
    "    data = result[1]\n",
    "    y_true_pred = pd.DataFrame({'y_actual': data['y_true'], 'y_predicted': data['y_pred']})\n",
    "\n",
    "    # create row of data with the calculations\n",
    "    true_pos = np.sum(y_true_pred['y_actual'] == 1)\n",
    "    true_neg = np.sum(y_true_pred['y_actual'] == 0)\n",
    "    pred_pos = np.sum(y_true_pred['y_predicted'] == 1)\n",
    "    false_pos_rate = np.sum((y_true_pred['y_actual'] == 0) & (y_true_pred['y_predicted'] == 1)) / (np.sum(y_true_pred['y_actual'] == 0))\n",
    "    false_neg_rate = np.sum((y_true_pred['y_actual'] == 1) & (y_true_pred['y_predicted'] == 0)) / (np.sum(y_true_pred['y_actual'] == 1))\n",
    "\n",
    "    # create a list of the stats to pass in\n",
    "    summary_stats = [true_pos, pred_pos, true_neg, len(y_true_pred) - pred_pos, false_pos_rate, false_neg_rate]\n",
    "\n",
    "    # insert lag name\n",
    "    summary_stats.insert(0, result[0])\n",
    "\n",
    "    # append to result list\n",
    "    iteration_summaries.append(summary_stats)\n",
    "\n",
    "# convert to df\n",
    "complete_summary_stats = pd.DataFrame(iteration_summaries, columns=headers_false_true_summary)\n",
    "\n",
    "# print results\n",
    "print(complete_summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b4ad29-60a4-4401-b616-9b9dca54ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up writer to export the summary stats\n",
    "path = '[ADD PATH TO EXPORT RESULTS]'\n",
    "writer = pd.ExcelWriter(path, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b318875-b2b8-4f14-b768-a1ae3e64da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to excel\n",
    "metric_data.to_excel(writer, sheet_name='summary_stats', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac86022e-cabe-452f-ae82-62de881a9380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add summary stats to excel output\n",
    "complete_summary_stats.to_excel(writer, sheet_name='pos_neg_acc_summary', index=False)\n",
    "# close writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1629219-1039-41ea-80c7-228a89c34025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new writer to export the raw predictions\n",
    "path_prob_raw = '../raw-pred/cm-pred-raw.xlsx'\n",
    "writer_raw_prob = pd.ExcelWriter(path_prob_raw, engine='openpyxl')\n",
    "# loop over all the models and get the raw prediction probabilities\n",
    "# as well as the y_true data for that loop\n",
    "for lag, res in zip(lags, consensus_results):\n",
    "    # get the results\n",
    "    res_data = res[1]\n",
    "    # concat into new df\n",
    "    res_df = pd.DataFrame({'y_true': res_data['y_true'], 'y_pred_raw': res_data['y_pred_raw']})\n",
    "    # export to new sheet\n",
    "    res_df.to_excel(writer_raw_prob, sheet_name=f\"{lag}_rf\", index=False)\n",
    "# close writer\n",
    "writer_raw_prob.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
